import requests
from bs4 import BeautifulSoup
import json
import config
from datetime import datetime, timedelta
import email.utils

def get_headers():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml, */*',
    }

def scrape_google_rss(source_name, query, days="1d"):
    """
    é€šç”¨å‡½æ•°ï¼šé€šè¿‡ Google News RSS æŠ“å– (é€‚åˆå‘¨æŠ¥/æœˆæŠ¥ï¼Œæœ‰å†å²æ•°æ®)
    """
    articles = []
    print(f"--- [Google RSS] æ­£åœ¨æŠ“å– {source_name} (è¿‡å» {days}) ---")
    
    rss_url = f"https://news.google.com/rss/search?q={query}+when:{days}&hl=en-ZA&gl=ZA&ceid=ZA:en"
    
    try:
        resp = requests.get(rss_url, headers=get_headers(), timeout=20)
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.content, 'xml')
            items = soup.find_all('item')
            
            for item in items:
                title = item.title.get_text(strip=True)
                # æ¸…ç† Google RSS æ ‡é¢˜ä¸­è‡ªå¸¦çš„ " - Source Name" åç¼€
                title = title.rsplit(' - ', 1)[0]
                link = item.link.get_text(strip=True)
                
                # ğŸ”¥ æ–°å¢ï¼šæå–æ‘˜è¦ (Google RSS çš„ description é€šå¸¸åŒ…å« HTMLï¼Œget_text ä¼šè‡ªåŠ¨æ¸…ç†æ ‡ç­¾)
                description = item.description.get_text(strip=True) if item.description else ""
                
                articles.append({
                    "source": source_name, 
                    "title": title, 
                    "link": link,
                    "description": description  # ä¿å­˜æ‘˜è¦
                })
            print(f"âœ… {source_name} (Google): è·å– {len(items)} æ¡")
        else:
            print(f"âŒ {source_name} (Google): è¯·æ±‚å¤±è´¥ Code {resp.status_code}")
            
    except Exception as e:
        print(f"âŒ {source_name} (Google) Error: {e}")
        
    return articles

def scrape_direct_rss(source_name, rss_url, days="1d"):
    """
    ä¸“ç”¨å‡½æ•°ï¼šç›´æ¥æŠ“å–å®˜æ–¹ RSS (é€‚åˆæ—¥æŠ¥ï¼Œæ— å»¶è¿Ÿ)
    åŒ…å«ä¸¥æ ¼çš„æ—¶é—´è¿‡æ»¤é€»è¾‘
    """
    articles = []
    print(f"--- [Direct RSS] æ­£åœ¨æŠ“å– {source_name} (å®˜æ–¹ç›´è¿ / è¿‡å» {days}) ---")
    
    # 1. è§£ææ—¶é—´èŒƒå›´
    try:
        days_int = int(days.replace("d", ""))
    except ValueError:
        days_int = 1

    # 2. è®¡ç®—æˆªæ­¢æ—¶é—´ (å½“å‰æ—¶é—´ - å¤©æ•°)
    cutoff_date = datetime.now().astimezone() - timedelta(days=days_int)
    
    try:
        resp = requests.get(rss_url, headers=get_headers(), timeout=15)
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.content, 'xml')
            items = soup.find_all('item')
            
            valid_count = 0
            for item in items: 
                # è§£æå‘å¸ƒæ—¶é—´
                pub_date_str = item.pubDate.get_text(strip=True) if item.pubDate else None
                
                is_within_range = False
                if pub_date_str:
                    try:
                        # è§£æ RSS æ—¶é—´ (RFC 822)
                        article_date = email.utils.parsedate_to_datetime(pub_date_str)
                        # å¤„ç†æ—¶åŒºé—®é¢˜
                        if article_date.tzinfo is None:
                             article_date = article_date.astimezone()
                             
                        # æ ¸å¿ƒè¿‡æ»¤é€»è¾‘ï¼šåªæœ‰æ™šäºæˆªæ­¢æ—¶é—´çš„æ‰ä¿ç•™
                        if article_date >= cutoff_date:
                            is_within_range = True
                    except Exception as e:
                        print(f"âš ï¸ æ—¥æœŸè§£æè­¦å‘Š: {e}")
                        is_within_range = True 
                
                if is_within_range:
                    title = item.title.get_text(strip=True)
                    link = item.link.get_text(strip=True)
                    
                    # ğŸ”¥ æ–°å¢ï¼šæå–æ‘˜è¦
                    description = item.description.get_text(strip=True) if item.description else ""
                    
                    articles.append({
                        "source": source_name, 
                        "title": title, 
                        "link": link,
                        "description": description # ä¿å­˜æ‘˜è¦
                    })
                    valid_count += 1
            
            print(f"âœ… {source_name} (Direct): è¿‡æ»¤åå‰©ä½™ {valid_count} æ¡ (å…± {len(items)} æ¡)")
        else:
            print(f"âŒ {source_name} (Direct): è¯·æ±‚å¤±è´¥ Code {resp.status_code}")
            return None # è¿”å› None è¡¨ç¤ºå¤±è´¥ï¼Œè§¦å‘ Fallback
            
    except Exception as e:
        print(f"âŒ {source_name} (Direct) Error: {e}")
        return None
        
    return articles

def scrape_all():
    all_articles = []
    
    current_days = config.TIME_RANGE  
    mode = config.REPORT_MODE
    print(f"ğŸš€ å¯åŠ¨çˆ¬è™« | æ¨¡å¼: {mode} | æ—¶é—´èŒƒå›´: {current_days}")

    # å®šä¹‰æ‰€æœ‰æºåŠå…¶é…ç½®
    sources = [
        {
            "name": "TechCentral", 
            "rss": "https://techcentral.co.za/feed/", 
            "google_query": "site:techcentral.co.za"
        },
        {
            "name": "MyBroadband", 
            "rss": "https://mybroadband.co.za/news/feed/", 
            "google_query": "site:mybroadband.co.za"
        },
        {
            "name": "ITWeb", 
            "rss": "https://www.itweb.co.za/rss", 
            "google_query": "site:itweb.co.za"
        }
    ]

    for src in sources:
        news_items = []
        
        # === æ™ºèƒ½ç­–ç•¥é€‰æ‹© ===
        if mode == "DAILY":
            news_items = scrape_direct_rss(src["name"], src["rss"], days=current_days)
        
        if news_items is None or (mode != "DAILY"):
            reason = "å‘¨/æœˆæŠ¥æ¨¡å¼" if mode != "DAILY" else "Direct RSS å¤±è´¥æˆ–ä¸ºç©º"
            print(f"ğŸ”„ åˆ‡æ¢åˆ° Google æº ({reason})...")
            news_items = scrape_google_rss(src["name"], src["google_query"], days=current_days)

        if news_items:
            all_articles.extend(news_items)

    # å»é‡é€»è¾‘ (ä»¥é“¾æ¥ä¸ºå‡†)
    unique_articles = []
    seen_links = set()
    for article in all_articles:
        if article['link'] not in seen_links:
            unique_articles.append(article)
            seen_links.add(article['link'])

    return unique_articles

if __name__ == "__main__":
    data = scrape_all()
    # å†™å…¥æ–‡ä»¶
    with open(config.RAW_NEWS_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ‰ çˆ¬è™«ç»“æŸï¼Œå…±ä¿å­˜ {len(data)} æ¡ã€‚")
