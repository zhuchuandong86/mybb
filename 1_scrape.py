import requests
from bs4 import BeautifulSoup
import json
import config
from datetime import datetime, timedelta
import email.utils

def get_headers():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml, */*',
    }

def scrape_google_rss(source_name, query, days="1d"):
    """
    é€šç”¨å‡½æ•°ï¼šé€šè¿‡ Google News RSS æŠ“å–æŒ‡å®šç½‘ç«™
    """
    articles = []
    print(f"--- æ­£åœ¨æŠ“å– {source_name} (Googleæ¸ é“ / è¿‡å» {days}) ---")
    
    rss_url = f"https://news.google.com/rss/search?q={query}+when:{days}&hl=en-ZA&gl=ZA&ceid=ZA:en"
    
    try:
        resp = requests.get(rss_url, headers=get_headers(), timeout=20)
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.content, 'xml')
            items = soup.find_all('item')
            
            for item in items:
                title = item.title.get_text(strip=True)
                title = title.replace(f" - {source_name}", "").replace(" - MyBroadband", "")
                link = item.link.get_text(strip=True)
                
                articles.append({
                    "source": source_name, 
                    "title": title, 
                    "link": link
                })
            print(f"âœ… {source_name}: æˆåŠŸè·å– {len(items)} æ¡")
        else:
            print(f"âŒ {source_name}: è¯·æ±‚å¤±è´¥ Code {resp.status_code}")
            
    except Exception as e:
        print(f"âŒ {source_name} Error: {e}")
        
    return articles

def scrape_direct_rss(source_name, rss_url):
    """
    ä¸“ç”¨å‡½æ•°ï¼šç›´æ¥æŠ“å–ç½‘ç«™å®˜æ–¹ RSS (è§£å†³ TechCentral Google æŠ“å–ä¸åˆ°çš„é—®é¢˜)
    """
    articles = []
    print(f"--- æ­£åœ¨æŠ“å– {source_name} (å®˜æ–¹ç›´è¿) ---")
    
    try:
        resp = requests.get(rss_url, headers=get_headers(), timeout=20)
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.content, 'xml')
            items = soup.find_all('item')
            
            # ç®€å•çš„æ—¥æœŸè¿‡æ»¤ï¼šåªå–å‰ 15 æ¡ï¼ˆé€šå¸¸åŒ…å«æœ€è¿‘3-5å¤©çš„æ–°é—»ï¼‰
            # è¿™é‡Œçš„ç›®çš„æ˜¯é˜²æ­¢æŠ“å–åˆ°å¤ªæ—§çš„æ–°é—»
            for item in items[:15]: 
                title = item.title.get_text(strip=True)
                link = item.link.get_text(strip=True)
                
                articles.append({
                    "source": source_name, 
                    "title": title, 
                    "link": link
                })
            print(f"âœ… {source_name}: æˆåŠŸè·å– {len(articles)} æ¡")
        else:
            print(f"âŒ {source_name}: è¯·æ±‚å¤±è´¥ Code {resp.status_code}")
    except Exception as e:
        print(f"âŒ {source_name} Error: {e}")
        
    return articles

def scrape_all():
    all_articles = []
    
    # === 1. ä½¿ç”¨ Google News æŠ“å–çš„æº (MyBB å’Œ ITWeb ç›®å‰æ­£å¸¸) ===
    google_sources = [
        {"name": "MyBroadband", "query": "site:mybroadband.co.za"},
        {"name": "ITWeb",       "query": "site:itweb.co.za"}
    ]

    for src in google_sources:
        news = scrape_google_rss(src["name"], src["query"], days="1d")
        all_articles.extend(news)

    # === 2. ä½¿ç”¨ å®˜æ–¹ç›´è¿ RSS æŠ“å–çš„æº (TechCentral) ===
    # TechCentral çš„å®˜æ–¹ RSS åœ°å€: https://techcentral.co.za/feed/
    tc_news = scrape_direct_rss("TechCentral", "https://techcentral.co.za/feed/")
    all_articles.extend(tc_news)

    # å»é‡é€»è¾‘
    unique_articles = []
    seen_links = set()
    for article in all_articles:
        if article['link'] not in seen_links:
            unique_articles.append(article)
            seen_links.add(article['link'])

    return unique_articles

if __name__ == "__main__":
    data = scrape_all()
    # å†™å…¥æ–‡ä»¶
    with open(config.RAW_NEWS_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ‰ çˆ¬è™«ç»“æŸï¼Œå…±ä¿å­˜ {len(data)} æ¡ã€‚")
