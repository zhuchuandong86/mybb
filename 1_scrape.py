import requests
from bs4 import BeautifulSoup
import json
import config
from datetime import datetime, timedelta
import email.utils

def get_headers():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml, */*',
    }

def scrape_google_rss(source_name, query, days="1d"):
    """
    é€šç”¨å‡½æ•°ï¼šé€šè¿‡ Google News RSS æŠ“å–æŒ‡å®šç½‘ç«™
    """
    articles = []
    print(f"--- æ­£åœ¨æŠ“å– {source_name} (Googleæ¸ é“ / è¿‡å» {days}) ---")
    
    rss_url = f"https://news.google.com/rss/search?q={query}+when:{days}&hl=en-ZA&gl=ZA&ceid=ZA:en"
    
    try:
        resp = requests.get(rss_url, headers=get_headers(), timeout=20)
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.content, 'xml')
            items = soup.find_all('item')
            
            for item in items:
                title = item.title.get_text(strip=True)
                title = title.replace(f" - {source_name}", "").replace(" - MyBroadband", "")
                link = item.link.get_text(strip=True)
                
                articles.append({
                    "source": source_name, 
                    "title": title, 
                    "link": link
                })
            print(f"âœ… {source_name}: æˆåŠŸè·å– {len(items)} æ¡")
        else:
            print(f"âŒ {source_name}: è¯·æ±‚å¤±è´¥ Code {resp.status_code}")
            
    except Exception as e:
        print(f"âŒ {source_name} Error: {e}")
        
    return articles

def scrape_direct_rss(source_name, rss_url, days="1d"):
    """
    ä¸“ç”¨å‡½æ•°ï¼šç›´æ¥æŠ“å–ç½‘ç«™å®˜æ–¹ RSS (è§£å†³ TechCentral Google æŠ“å–ä¸åˆ°çš„é—®é¢˜)
    ğŸ”¥ ä¿®æ”¹ï¼šå¢åŠ æ—¶é—´è¿‡æ»¤é€»è¾‘ï¼Œç²¾å‡†æ§åˆ¶æ—¶é—´èŒƒå›´
    """
    articles = []
    print(f"--- æ­£åœ¨æŠ“å– {source_name} (å®˜æ–¹ç›´è¿ / è¿‡å» {days}) ---")
    
    # 1. è§£ææ—¶é—´èŒƒå›´ (ä¾‹å¦‚ "1d" -> 1, "7d" -> 7)
    try:
        days_int = int(days.replace("d", ""))
    except ValueError:
        days_int = 1

    # 2. è®¡ç®—æˆªæ­¢æ—¶é—´ (ä½¿ç”¨å½“å‰æ—¶åŒºæ—¶é—´)
    cutoff_date = datetime.now().astimezone() - timedelta(days=days_int)
    
    try:
        resp = requests.get(rss_url, headers=get_headers(), timeout=20)
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.content, 'xml')
            items = soup.find_all('item')
            
            # ä¸å†ç¡¬æ€§æˆªå–å‰15æ¡ï¼Œè€Œæ˜¯éå†æ‰€æœ‰æ¡ç›®è¿›è¡Œæ—¶é—´åˆ¤æ–­
            count = 0
            for item in items: 
                # è§£æå‘å¸ƒæ—¶é—´
                pub_date_str = item.pubDate.get_text(strip=True) if item.pubDate else None
                
                is_within_range = False
                if pub_date_str:
                    try:
                        # è§£æ RSS æ—¶é—´ (RFC 822)
                        article_date = email.utils.parsedate_to_datetime(pub_date_str)
                        
                        # å¦‚æœ article_date æ²¡å¸¦æ—¶åŒºï¼Œå‡å®šä¸ºå½“å‰æ—¶åŒºï¼ˆé˜²æ­¢æŠ¥é”™ï¼‰
                        if article_date.tzinfo is None:
                             article_date = article_date.astimezone()
                             
                        # æ¯”è¾ƒæ—¶é—´
                        if article_date >= cutoff_date:
                            is_within_range = True
                    except Exception as e:
                        print(f"âš ï¸ æ—¥æœŸè§£æé”™è¯¯: {pub_date_str} - {e}")
                
                # å¦‚æœåœ¨æ—¶é—´èŒƒå›´å†…ï¼Œåˆ™åŠ å…¥
                if is_within_range:
                    title = item.title.get_text(strip=True)
                    link = item.link.get_text(strip=True)
                    
                    articles.append({
                        "source": source_name, 
                        "title": title, 
                        "link": link
                    })
                    count += 1
            
            print(f"âœ… {source_name}: æˆåŠŸè·å– {count} æ¡ (è¿‡æ»¤å)")
        else:
            print(f"âŒ {source_name}: è¯·æ±‚å¤±è´¥ Code {resp.status_code}")
    except Exception as e:
        print(f"âŒ {source_name} Error: {e}")
        
    return articles

def scrape_all():
    all_articles = []
    
    # ä» config è¯»å–æ—¶é—´èŒƒå›´
    current_days = config.TIME_RANGE  
    print(f"å½“å‰è¿è¡Œæ¨¡å¼: {config.REPORT_MODE}, æŠ“å–èŒƒå›´: {current_days}")

    # === 1. Google News æº ===
    google_sources = [
        {"name": "MyBroadband", "query": "site:mybroadband.co.za"},
        {"name": "ITWeb",       "query": "site:itweb.co.za"}
    ]

    for src in google_sources:
        news = scrape_google_rss(src["name"], src["query"], days=current_days)
        all_articles.extend(news)

    # === 2. å®˜æ–¹æº TechCentral ===
    # ğŸ”¥ ä¿®æ”¹ç‚¹ï¼šä¼ å…¥ days=current_days å‚æ•°
    tc_news = scrape_direct_rss("TechCentral", "https://techcentral.co.za/feed/", days=current_days)
  
    all_articles.extend(tc_news)

    # å»é‡é€»è¾‘
    unique_articles = []
    seen_links = set()
    for article in all_articles:
        if article['link'] not in seen_links:
            unique_articles.append(article)
            seen_links.add(article['link'])

    return unique_articles

if __name__ == "__main__":
    data = scrape_all()
    # å†™å…¥æ–‡ä»¶
    with open(config.RAW_NEWS_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ‰ çˆ¬è™«ç»“æŸï¼Œå…±ä¿å­˜ {len(data)} æ¡ã€‚")
